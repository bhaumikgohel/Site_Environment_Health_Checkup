# ğŸš€ Vercel Deployment Guide with Ollama LLM 3.2

This guide will help you deploy the Environment Health Checkup application to Vercel with AI-powered analysis using Ollama LLM 3.2.

## ğŸ“‹ Prerequisites

1. **Vercel Account**: Sign up at [vercel.com](https://vercel.com)
2. **Vercel CLI**: Install globally
   ```bash
   npm install -g vercel
   ```
3. **Ollama** (for local AI features): Install from [ollama.com](https://ollama.com)

## ğŸ› ï¸ Setup

### 1. Install Dependencies

```bash
npm install
```

### 2. Configure Environment Variables

Create a `.env` file in the root directory:

```env
# Ollama Configuration (for local AI analysis)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2
```

## ğŸ“¦ Project Structure for Vercel

```
â”œâ”€â”€ api/                    # Serverless API Functions
â”‚   â”œâ”€â”€ _utils.js          # Shared utilities
â”‚   â”œâ”€â”€ configs.js         # Get configurations
â”‚   â”œâ”€â”€ save-config.js     # Save configuration
â”‚   â”œâ”€â”€ history.js         # Get history
â”‚   â”œâ”€â”€ save-history.js    # Save history
â”‚   â”œâ”€â”€ archive.js         # Get archive
â”‚   â”œâ”€â”€ archive-config.js  # Archive config
â”‚   â”œâ”€â”€ run-check.js       # Run health check
â”‚   â”œâ”€â”€ ai-analyze.js      # ğŸ¤– Ollama LLM AI Analysis
â”‚   â””â”€â”€ ollama-status.js   # ğŸ¤– Ollama Status Check
â”œâ”€â”€ public/                # Static Assets
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ style.css          # Includes AI Analysis styles
â”‚   â””â”€â”€ app.js             # Includes AI Analysis UI
â”œâ”€â”€ vercel.json            # Vercel configuration
â””â”€â”€ package.json
```

## ğŸš€ Deployment Steps

### Option 1: Deploy via Vercel CLI (Recommended)

1. **Login to Vercel**
   ```bash
   vercel login
   ```

2. **Deploy to Preview**
   ```bash
   vercel
   ```

3. **Deploy to Production**
   ```bash
   vercel --prod
   ```

### Option 2: Deploy via Git Integration

1. Push your code to GitHub/GitLab/Bitbucket
2. Connect your repository on [Vercel Dashboard](https://vercel.com/dashboard)
3. Vercel will auto-deploy on every push

## ğŸ¤– Ollama LLM 3.2 Integration

### Setting up Ollama (Local)

The AI analysis feature requires Ollama to be running locally:

1. **Install Ollama**
   Download from [ollama.com](https://ollama.com)

2. **Download Llama 3.2 Model**
   ```bash
   ollama pull llama3.2
   ```

3. **Start Ollama Server**
   ```bash
   ollama serve
   ```

4. **Verify Installation**
   ```bash
   ollama run llama3.2
   ```

### How AI Analysis Works

1. Run a health check in the dashboard
2. Click the **ğŸ¤– AI Analysis** button
3. The app sends health check data to your local Ollama instance
4. LLM 3.2 analyzes the results and provides:
   - Overall health assessment
   - Critical issues identification
   - Recommendations for improvement
   - Performance observations

### Important Note on Ollama

âš ï¸ **Ollama runs locally** - The AI analysis feature requires Ollama to be running on your local machine. The deployed Vercel app will connect to your local Ollama instance via:

- **Local development**: `http://localhost:11434`
- **Production with tunnel**: Use tools like [ngrok](https://ngrok.com) to expose your local Ollama

```bash
# Expose Ollama via ngrok (for remote access)
ngrok http 11434
```

Then set the `OLLAMA_HOST` environment variable in Vercel to your ngrok URL.

## ğŸ”§ Environment Variables on Vercel

Set these in your Vercel project settings:

| Variable | Description | Default |
|----------|-------------|---------|
| `OLLAMA_HOST` | Ollama server URL | `http://localhost:11434` |
| `OLLAMA_MODEL` | Model name | `llama3.2` |

## ğŸŒ Post-Deployment

### Access Your Application

After deployment, Vercel will provide you with a URL like:
```
https://your-project.vercel.app
```

### Features Available Online

âœ… **Full Web Dashboard**  
âœ… **Configuration Management**  
âœ… **Run History Tracking**  ï¸
âœ… **Health Check (Basic)**  
âœ… **HTML Report Download**  
ğŸ¤– **AI Analysis** (requires local Ollama)  

### Limitations on Vercel

âš ï¸ **Playwright Limitation**: Full browser automation with Playwright is not supported in Vercel's serverless environment. The app uses simulated health checks for:
- URL accessibility
- API endpoint checks

For full Playwright functionality, consider deploying to:
- Railway
- Render
- AWS EC2
- DigitalOcean Droplets

## ğŸ”„ Local Development

```bash
# Install dependencies
npm install

# Run locally (uses full Playwright)
node server.js

# Or use Vercel dev server
vercel dev
```

## ğŸ› ï¸ Troubleshooting

### Ollama Connection Issues

**Problem**: AI Analysis shows "AI Offline"

**Solution**:
1. Ensure Ollama is running: `ollama serve`
2. Check if model is downloaded: `ollama list`
3. Verify model: `ollama run llama3.2`
4. Check firewall settings for port 11434

### Vercel Deployment Fails

**Problem**: Build errors

**Solution**:
```bash
# Update Vercel CLI
npm update -g vercel

# Clear cache and redeploy
vercel --force
```

### CORS Issues

The API routes include CORS headers. If you encounter issues:

1. Check `vercel.json` routes configuration
2. Ensure API files use the `_utils.getCorsHeaders()` function

## ğŸ“š Additional Resources

- [Vercel Documentation](https://vercel.com/docs)
- [Ollama Documentation](https://github.com/ollama/ollama)
- [Llama 3.2 Model](https://ollama.com/library/llama3.2)

## ğŸ“ Summary

âœ… **Deployed Features**:
- Premium web dashboard with glassmorphism UI
- Multi-environment configuration
- Health check monitoring
- Run history tracking
- AI-powered analysis (with local Ollama)

ğŸ¤– **Ollama Integration**:
- Local LLM 3.2 processing for privacy
- Intelligent health report analysis
- Markdown-formatted insights
- Real-time status checking

---

**Need help?** Open an issue or check the troubleshooting section above.
